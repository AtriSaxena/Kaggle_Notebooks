{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b1d3c46",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.019493,
     "end_time": "2022-10-29T16:39:59.692571",
     "exception": false,
     "start_time": "2022-10-29T16:39:59.673078",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <div style=\"font-family: Trebuchet MS; background-color: #D68910; color: #FFFFFF; padding: 12px; line-height: 1.5;border-radius:15px 60px 15px; text-align:center;border-color: red;border: 5px solid #17b978;overflow:hidden;\">1. | Introduction  ⌛</div>\n",
    "\n",
    "DTensor, an extension to TensorFlow for synchronous distributed computing.\n",
    "\n",
    "DTensor provides a global programming model that allows developers to compose applications that operate on Tensors globally while managing the distribution across devices internally. DTensor distributes the program and tensors according to the sharding directives through a procedure called Single program, multiple data (SPMD) expansion.\n",
    "\n",
    "By decoupling the application from sharding directives, DTensor enables running the same application on a single device, multiple devices, or even multiple clients, while preserving its global semantics.\n",
    "\n",
    "This guide introduces DTensor concepts for distributed computing, and how DTensor integrates with TensorFlow. To see a demo of using DTensor in model training, see Distributed training with DTensor tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135e2191",
   "metadata": {
    "papermill": {
     "duration": 0.014752,
     "end_time": "2022-10-29T16:39:59.722749",
     "exception": false,
     "start_time": "2022-10-29T16:39:59.707997",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "> <div style=\"font-weight:bold;font-size:20px;line-height:1.5;padding:12px;background-color: #f9ff21;font-family: Verdana, Cursive; color: #ff1f5a;border: 5px solid #17b978;\">\n",
    "    \"Sharding here simple means dividing large part(Tensor) into smaller parts (Tensor) on various distributed devices.\" \n",
    "    <br>\n",
    "    <div> The word shard means \"a small part of a whole.\"</div>\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3fef1fe",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2022-10-29T16:39:59.754911Z",
     "iopub.status.busy": "2022-10-29T16:39:59.754280Z",
     "iopub.status.idle": "2022-10-29T16:41:43.729939Z",
     "shell.execute_reply": "2022-10-29T16:41:43.725181Z"
    },
    "papermill": {
     "duration": 104.003018,
     "end_time": "2022-10-29T16:41:43.740759",
     "exception": false,
     "start_time": "2022-10-29T16:39:59.737741",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\r\n",
      "\r\n",
      "\u001b[1;31mE: \u001b[0mUnable to locate package libcudnn8\u001b[0m\r\n",
      "Collecting tensorflow==2.9.0\r\n",
      "  Downloading tensorflow-2.9.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.7 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m511.7/511.7 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting libclang>=13.0.0\r\n",
      "  Downloading libclang-14.0.6-py2.py3-none-manylinux2010_x86_64.whl (14.1 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.9.0) (1.15.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.9.0) (4.4.0)\r\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\r\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.27.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.9.0) (3.3.0)\r\n",
      "Collecting absl-py>=1.0.0\r\n",
      "  Downloading absl_py-1.3.0-py3-none-any.whl (124 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.6/124.6 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting tensorboard<2.10,>=2.9\r\n",
      "  Downloading tensorboard-2.9.1-py3-none-any.whl (5.8 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.9.0) (0.2.0)\r\n",
      "Requirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.9.0) (3.7.0)\r\n",
      "Requirement already satisfied: flatbuffers<2,>=1.12 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.9.0) (1.12)\r\n",
      "Requirement already satisfied: numpy>=1.20 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.9.0) (1.21.6)\r\n",
      "Collecting tensorflow-estimator<2.10.0,>=2.9.0rc0\r\n",
      "  Downloading tensorflow_estimator-2.9.0-py2.py3-none-any.whl (438 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.7/438.7 kB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.9.0) (1.12.1)\r\n",
      "Collecting keras<2.10.0,>=2.9.0rc0\r\n",
      "  Downloading keras-2.9.0-py2.py3-none-any.whl (1.6 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.9.0) (59.8.0)\r\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.9.0) (1.1.0)\r\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.9.0) (1.6.3)\r\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.9.0) (0.4.0)\r\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.9.0) (1.1.2)\r\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.9.0) (1.43.0)\r\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.9.0) (21.3)\r\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.9.0) (3.19.4)\r\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.7/site-packages (from astunparse>=1.6.0->tensorflow==2.9.0) (0.37.1)\r\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.0) (1.8.1)\r\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.0) (3.3.7)\r\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.0) (2.28.1)\r\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.0) (2.2.2)\r\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.0) (0.6.1)\r\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.0) (1.35.0)\r\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.0) (0.4.6)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->tensorflow==2.9.0) (3.0.9)\r\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (0.2.7)\r\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (4.2.4)\r\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (4.8)\r\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (1.3.1)\r\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (4.13.0)\r\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (2.1.0)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (3.3)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (1.26.12)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (2022.9.24)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.7/site-packages (from werkzeug>=1.0.1->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (2.1.1)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (3.8.0)\r\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (0.4.8)\r\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (3.2.0)\r\n",
      "Installing collected packages: libclang, keras, tensorflow-io-gcs-filesystem, tensorflow-estimator, absl-py, tensorboard, tensorflow\r\n",
      "  Attempting uninstall: keras\r\n",
      "    Found existing installation: keras 2.6.0\r\n",
      "    Uninstalling keras-2.6.0:\r\n",
      "      Successfully uninstalled keras-2.6.0\r\n",
      "  Attempting uninstall: tensorflow-estimator\r\n",
      "    Found existing installation: tensorflow-estimator 2.6.0\r\n",
      "    Uninstalling tensorflow-estimator-2.6.0:\r\n",
      "      Successfully uninstalled tensorflow-estimator-2.6.0\r\n",
      "  Attempting uninstall: absl-py\r\n",
      "    Found existing installation: absl-py 0.15.0\r\n",
      "    Uninstalling absl-py-0.15.0:\r\n",
      "      Successfully uninstalled absl-py-0.15.0\r\n",
      "  Attempting uninstall: tensorboard\r\n",
      "    Found existing installation: tensorboard 2.10.1\r\n",
      "    Uninstalling tensorboard-2.10.1:\r\n",
      "      Successfully uninstalled tensorboard-2.10.1\r\n",
      "  Attempting uninstall: tensorflow\r\n",
      "    Found existing installation: tensorflow 2.6.4\r\n",
      "    Uninstalling tensorflow-2.6.4:\r\n",
      "      Successfully uninstalled tensorflow-2.6.4\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "tfx-bsl 1.9.0 requires pyarrow<6,>=1, but you have pyarrow 8.0.0 which is incompatible.\r\n",
      "tensorflow-transform 1.9.0 requires pyarrow<6,>=1, but you have pyarrow 8.0.0 which is incompatible.\r\n",
      "tensorflow-io 0.21.0 requires tensorflow<2.7.0,>=2.6.0, but you have tensorflow 2.9.0 which is incompatible.\r\n",
      "tensorflow-io 0.21.0 requires tensorflow-io-gcs-filesystem==0.21.0, but you have tensorflow-io-gcs-filesystem 0.27.0 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed absl-py-1.3.0 keras-2.9.0 libclang-14.0.6 tensorboard-2.9.1 tensorflow-2.9.0 tensorflow-estimator-2.9.0 tensorflow-io-gcs-filesystem-0.27.0\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!apt -y install --allow-change-held-packages libcudnn8=8.1.0.77-1+cuda11.2\n",
    "!pip install tensorflow==2.9.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f8c7cf9",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2022-10-29T16:41:43.892652Z",
     "iopub.status.busy": "2022-10-29T16:41:43.891183Z",
     "iopub.status.idle": "2022-10-29T16:41:43.916927Z",
     "shell.execute_reply": "2022-10-29T16:41:43.915940Z"
    },
    "papermill": {
     "duration": 0.101599,
     "end_time": "2022-10-29T16:41:43.920206",
     "exception": false,
     "start_time": "2022-10-29T16:41:43.818607",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style> \n",
       "\n",
       "div #notebook {\n",
       "background-color: white;\n",
       "line-height: 20px;\n",
       "}\n",
       "\n",
       "#notebook-container {\n",
       "width: 95%;\n",
       "margin-top: 2em;\n",
       "padding-top: 2em;\n",
       "border-top: 4px solid #16a085; /* light orange */\n",
       "-webkit-box-shadow: 0px 0px 8px 2px rgba(224, 212, 226, 0.5); /* pink */\n",
       "    box-shadow: 0px 0px 8px 2px rgba(224, 212, 226, 0.5); /* pink */\n",
       "}\n",
       "\n",
       "div .input {\n",
       "margin-bottom: 1em;\n",
       "}\n",
       "\n",
       ".rendered_html h1, .rendered_html h2, .rendered_html h3, .rendered_html h4, .rendered_html h5, .rendered_html h6 {\n",
       "color: #16a085; /* light orange */\n",
       "font-weight: 600;\n",
       "}\n",
       "\n",
       "div.input_area {\n",
       "border: none;\n",
       "    background-color: rgba(22, 160, 133, 0.1); /* rgba(229, 143, 101, 0.1); light orange [exactly #E58F65] */\n",
       "    border-top: 2px solid #16a085; /* light orange */\n",
       "}\n",
       "\n",
       "div.input_prompt {\n",
       "color: #16a085; /* light blue */\n",
       "}\n",
       "\n",
       "div.output_prompt {\n",
       "color: #0b5345; /* strong orange */\n",
       "}\n",
       "\n",
       "div.cell.selected:before, div.cell.selected.jupyter-soft-selected:before {\n",
       "background: #16a085; /* light orange */\n",
       "}\n",
       "\n",
       "div.cell.selected, div.cell.selected.jupyter-soft-selected {\n",
       "    border-color: #16a085; /* light orange */\n",
       "}\n",
       "\n",
       ".edit_mode div.cell.selected:before {\n",
       "background: #16a085; /* light orange */\n",
       "}\n",
       "\n",
       ".edit_mode div.cell.selected {\n",
       "border-color: #16a085; /* light orange */\n",
       "\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML, Javascript\n",
    "\n",
    "# ----- Notebook Theme -----\n",
    "color_map = ['#16a085', '#e8f6f3', '#d0ece7', '#a2d9ce', '#73c6b6', '#45b39d', \n",
    "                        '#16a085', '#138d75', '#117a65', '#0e6655', '#0b5345']\n",
    "\n",
    "prompt = color_map[-1]\n",
    "main_color = color_map[0]\n",
    "strong_main_color = color_map[1]\n",
    "custom_colors = [strong_main_color, main_color]\n",
    "\n",
    "css_file = ''' \n",
    "\n",
    "div #notebook {\n",
    "background-color: white;\n",
    "line-height: 20px;\n",
    "}\n",
    "\n",
    "#notebook-container {\n",
    "%s\n",
    "margin-top: 2em;\n",
    "padding-top: 2em;\n",
    "border-top: 4px solid %s; /* light orange */\n",
    "-webkit-box-shadow: 0px 0px 8px 2px rgba(224, 212, 226, 0.5); /* pink */\n",
    "    box-shadow: 0px 0px 8px 2px rgba(224, 212, 226, 0.5); /* pink */\n",
    "}\n",
    "\n",
    "div .input {\n",
    "margin-bottom: 1em;\n",
    "}\n",
    "\n",
    ".rendered_html h1, .rendered_html h2, .rendered_html h3, .rendered_html h4, .rendered_html h5, .rendered_html h6 {\n",
    "color: %s; /* light orange */\n",
    "font-weight: 600;\n",
    "}\n",
    "\n",
    "div.input_area {\n",
    "border: none;\n",
    "    background-color: %s; /* rgba(229, 143, 101, 0.1); light orange [exactly #E58F65] */\n",
    "    border-top: 2px solid %s; /* light orange */\n",
    "}\n",
    "\n",
    "div.input_prompt {\n",
    "color: %s; /* light blue */\n",
    "}\n",
    "\n",
    "div.output_prompt {\n",
    "color: %s; /* strong orange */\n",
    "}\n",
    "\n",
    "div.cell.selected:before, div.cell.selected.jupyter-soft-selected:before {\n",
    "background: %s; /* light orange */\n",
    "}\n",
    "\n",
    "div.cell.selected, div.cell.selected.jupyter-soft-selected {\n",
    "    border-color: %s; /* light orange */\n",
    "}\n",
    "\n",
    ".edit_mode div.cell.selected:before {\n",
    "background: %s; /* light orange */\n",
    "}\n",
    "\n",
    ".edit_mode div.cell.selected {\n",
    "border-color: %s; /* light orange */\n",
    "\n",
    "}\n",
    "'''\n",
    "def to_rgb(h): \n",
    "    return tuple(int(h[i:i+2], 16) for i in [0, 2, 4])\n",
    "\n",
    "main_color_rgba = 'rgba(%s, %s, %s, 0.1)' % (to_rgb(main_color[1:]))\n",
    "open('notebook.css', 'w').write(css_file % ('width: 95%;', main_color, main_color, main_color_rgba, main_color,  main_color, prompt, main_color, main_color, main_color, main_color))\n",
    "\n",
    "def nb(): \n",
    "    return HTML(\"<style>\" + open(\"notebook.css\", \"r\").read() + \"</style>\")\n",
    "nb()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42190183",
   "metadata": {
    "papermill": {
     "duration": 0.067959,
     "end_time": "2022-10-29T16:41:44.055674",
     "exception": false,
     "start_time": "2022-10-29T16:41:43.987715",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <div style=\"font-family: Trebuchet MS; background-color: #D68910; color: #FFFFFF; padding: 12px; line-height: 1.5;border-radius:15px 60px 15px; text-align:center;border-color: red;border: 5px solid #17b978;overflow:hidden;\">2. | Importing Libraries  ⌛</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30dba7ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-29T16:41:44.193832Z",
     "iopub.status.busy": "2022-10-29T16:41:44.192987Z",
     "iopub.status.idle": "2022-10-29T16:41:49.896542Z",
     "shell.execute_reply": "2022-10-29T16:41:49.895158Z"
    },
    "papermill": {
     "duration": 5.776379,
     "end_time": "2022-10-29T16:41:49.899825",
     "exception": false,
     "start_time": "2022-10-29T16:41:44.123446",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-29 16:41:44.407805: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda/lib:/usr/local/lib/x86_64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:\n",
      "2022-10-29 16:41:44.407902: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.experimental import dtensor\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9751621",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-29T16:41:50.048589Z",
     "iopub.status.busy": "2022-10-29T16:41:50.047334Z",
     "iopub.status.idle": "2022-10-29T16:41:50.054252Z",
     "shell.execute_reply": "2022-10-29T16:41:50.053175Z"
    },
    "papermill": {
     "duration": 0.083821,
     "end_time": "2022-10-29T16:41:50.056452",
     "exception": false,
     "start_time": "2022-10-29T16:41:49.972631",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.9.0\n"
     ]
    }
   ],
   "source": [
    "print('TensorFlow version:', tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3946e3",
   "metadata": {
    "papermill": {
     "duration": 0.069335,
     "end_time": "2022-10-29T16:41:50.195360",
     "exception": false,
     "start_time": "2022-10-29T16:41:50.126025",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <div style=\"font-family: Trebuchet MS; background-color: #D68910; color: #FFFFFF; padding: 12px; line-height: 1.5;border-radius:15px 60px 15px; text-align:center;border-color: red;border: 5px solid #17b978;overflow:hidden;\">3. | Setup vCPU ⌛</div>\n",
    "\n",
    "Then configure TensorFlow to use 6 virtual CPUs.\n",
    "\n",
    "Even though this example uses vCPUs, DTensor works the same way on CPU, GPU or TPU devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb32fd42",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-29T16:41:50.340717Z",
     "iopub.status.busy": "2022-10-29T16:41:50.340015Z",
     "iopub.status.idle": "2022-10-29T16:41:50.354782Z",
     "shell.execute_reply": "2022-10-29T16:41:50.353757Z"
    },
    "papermill": {
     "duration": 0.088555,
     "end_time": "2022-10-29T16:41:50.357037",
     "exception": false,
     "start_time": "2022-10-29T16:41:50.268482",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-29 16:41:50.344813: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda/lib:/usr/local/lib/x86_64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:\n",
      "2022-10-29 16:41:50.344877: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-10-29 16:41:50.344908: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (5d70d7980043): /proc/driver/nvidia/version does not exist\n",
      "2022-10-29 16:41:50.346006: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[LogicalDevice(name='/device:CPU:0', device_type='CPU'),\n",
       " LogicalDevice(name='/device:CPU:1', device_type='CPU'),\n",
       " LogicalDevice(name='/device:CPU:2', device_type='CPU'),\n",
       " LogicalDevice(name='/device:CPU:3', device_type='CPU'),\n",
       " LogicalDevice(name='/device:CPU:4', device_type='CPU'),\n",
       " LogicalDevice(name='/device:CPU:5', device_type='CPU')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def configure_virtual_cpus(ncpu):\n",
    "  phy_devices = tf.config.list_physical_devices('CPU')\n",
    "  tf.config.set_logical_device_configuration(phy_devices[0], [\n",
    "        tf.config.LogicalDeviceConfiguration(),\n",
    "    ] * ncpu)\n",
    "\n",
    "configure_virtual_cpus(6)\n",
    "DEVICES = [f'CPU:{i}' for i in range(6)]\n",
    "\n",
    "tf.config.list_logical_devices('CPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab4f73f",
   "metadata": {
    "papermill": {
     "duration": 0.069467,
     "end_time": "2022-10-29T16:41:50.496858",
     "exception": false,
     "start_time": "2022-10-29T16:41:50.427391",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <div style=\"font-family: Trebuchet MS; background-color: #D68910; color: #FFFFFF; padding: 12px; line-height: 1.5;border-radius:15px 60px 15px; text-align:center;border-color: red;border: 5px solid #17b978;overflow:hidden;\">4. | DTensor Model </div>\n",
    "\n",
    "**Let's try to understand about DTensor:** \n",
    "\n",
    "DTensor introduces two concepts: `dtensor.Mesh` and `dtensor.Layout`.\n",
    "- `Mesh` defines the device list for computation.\n",
    "- `Layout` defines how to shard the Tensor dimension on a `Mesh`.\n",
    "\n",
    "![image.png](https://i.imgur.com/a8No8iz.png)\n",
    "\n",
    "**NOTE:** ***`Using this image as a reference & try to understand concept explained below.`***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94ab9e4",
   "metadata": {
    "papermill": {
     "duration": 0.072388,
     "end_time": "2022-10-29T16:41:50.642546",
     "exception": false,
     "start_time": "2022-10-29T16:41:50.570158",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "### <div style=\"font-weight:bold;line-height:1.5;padding:12px;background-color: #D68910;font-family: Trebuchet MS; color: #FFFFFF;border-radius:15px 60px 15px;border: 5px solid #17b978;text-align:center;\">4.1 | Mesh ⚡</div> \n",
    "\n",
    "Mesh is a logical Cartisian topology of a set of devices. Each dimension of the Cartisian grid is called a **Mesh dimension**, and referred to with a name. Names of mesh dimension within the same `Mesh` must be unique.\n",
    "\n",
    "Names of mesh dimensions are referenced by `Layout` to describe the sharding behavior of a `tf.Tensor` along each of its axes. This is described in more detail later in the section on `Layout`.\n",
    "\n",
    "`Mesh` can be thought of as a multi-dimensional array of devices.\n",
    "\n",
    "In a 1 dimensional `Mesh`, all devices form a list in a single mesh dimension. The following example uses `dtensor.create_mesh` to create a mesh from 6 CPU devices along a mesh dimension `'x'` with a size of 6 devices:\n",
    "\n",
    "<img src=\"https://www.tensorflow.org/images/dtensor/dtensor_mesh_1d.png\" alt=\"A 1 dimensional mesh with 6 CPUs\" class=\"no-filter\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c15f39d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-29T16:41:50.788302Z",
     "iopub.status.busy": "2022-10-29T16:41:50.787229Z",
     "iopub.status.idle": "2022-10-29T16:41:50.802010Z",
     "shell.execute_reply": "2022-10-29T16:41:50.800616Z"
    },
    "papermill": {
     "duration": 0.092192,
     "end_time": "2022-10-29T16:41:50.805182",
     "exception": false,
     "start_time": "2022-10-29T16:41:50.712990",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.dtensor.python.layout.Mesh object at 0x7f8243da8c90>\n"
     ]
    }
   ],
   "source": [
    "mesh_1d = dtensor.create_mesh([('x', 6)], devices=DEVICES)\n",
    "print(mesh_1d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795bae2b",
   "metadata": {
    "papermill": {
     "duration": 0.08194,
     "end_time": "2022-10-29T16:41:50.979614",
     "exception": false,
     "start_time": "2022-10-29T16:41:50.897674",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "A `Mesh` can be multi dimensional as well. In the following example, 6 CPU devices form a `3x2` mesh, where the `'x'` mesh dimension has a size of 3 devices, and the `'y'` mesh dimension has a size of 2 devices:\n",
    "\n",
    "<img src=\"https://www.tensorflow.org/images/dtensor/dtensor_mesh_2d.png\" alt=\"A 2 dimensional mesh with 6 CPUs\"\n",
    "     class=\"no-filter\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89b226bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-29T16:41:51.146682Z",
     "iopub.status.busy": "2022-10-29T16:41:51.146281Z",
     "iopub.status.idle": "2022-10-29T16:41:51.154870Z",
     "shell.execute_reply": "2022-10-29T16:41:51.153627Z"
    },
    "papermill": {
     "duration": 0.097404,
     "end_time": "2022-10-29T16:41:51.159388",
     "exception": false,
     "start_time": "2022-10-29T16:41:51.061984",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.dtensor.python.layout.Mesh object at 0x7f8243d9fd50>\n"
     ]
    }
   ],
   "source": [
    "mesh_2d = dtensor.create_mesh([('x', 3), ('y', 2)], devices=DEVICES)\n",
    "print(mesh_2d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf9e346",
   "metadata": {
    "papermill": {
     "duration": 0.071503,
     "end_time": "2022-10-29T16:41:51.314013",
     "exception": false,
     "start_time": "2022-10-29T16:41:51.242510",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### <div style=\"font-weight:bold;line-height:1.5;padding:12px;background-color: #D68910;font-family: Trebuchet MS; color: #FFFFFF;border-radius:15px 60px 15px;border: 5px solid #17b978;text-align:center\">4.2 | Layout </div> \n",
    "\n",
    "**`Layout`** specifies how a tensor is distributed, or sharded, on a `Mesh`.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ccb198",
   "metadata": {
    "papermill": {
     "duration": 0.071045,
     "end_time": "2022-10-29T16:41:51.461547",
     "exception": false,
     "start_time": "2022-10-29T16:41:51.390502",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"font-weight:normal; font-size:18px;line-height:1.0;padding:12px;background-color: #f9ff21;font-family: Trebuchet MS; color: #ff1f5a;border: 5px solid #17b978;\"><b>Note:</b><br><br> In order to avoid confusions between <code>Mesh</code> and <code>Layout</code>, the term <b>\"dimension\"</b> is always associated with <code>Mesh</code>, and the term <b>\"axis\"</b> with <code>Tensor</code> and <code>Layout</code> in this guide.</div> \n",
    "\n",
    "The rank of `Layout` should be the same as the rank of the `Tensor` where the `Layout` is applied. For each of the `Tensor`'s axes the `Layout` may specify a mesh dimension to shard the tensor across, or specify the axis as \"unsharded\".\n",
    "The tensor is replicated across any mesh dimensions that it is not sharded across.\n",
    "\n",
    "The rank of a `Layout` and the number of dimensions of a `Mesh` do not need to match. The `unsharded` axes of a `Layout` do not need to be associated to a mesh dimension, and `unsharded` mesh dimensions do not need to be associated with a `layout` axis.\n",
    "\n",
    "<img src=\"https://www.tensorflow.org/images/dtensor/dtensor_components_diag.png\" alt=\"Diagram of dtensor components.\"\n",
    "     class=\"no-filter\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df52063",
   "metadata": {
    "papermill": {
     "duration": 0.105659,
     "end_time": "2022-10-29T16:41:51.645654",
     "exception": false,
     "start_time": "2022-10-29T16:41:51.539995",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Let's analyze a few examples of `Layout` for the `Mesh`'s created in the previous section.**\n",
    "\n",
    "On a 1-dimensional mesh such as `[(\"x\", 6)]` (`mesh_1d` in the previous section), `Layout([\"unsharded\", \"unsharded\"], mesh_1d)` is a layout for a rank-2 tensor replicated across 6 devices.\n",
    "<img src=\"https://www.tensorflow.org/images/dtensor/dtensor_layout_replicated.png\" alt=\"A tensor replicated across a rank-1 mesh\" class=\"no-filter\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0792da3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-29T16:41:51.912221Z",
     "iopub.status.busy": "2022-10-29T16:41:51.911402Z",
     "iopub.status.idle": "2022-10-29T16:41:51.916778Z",
     "shell.execute_reply": "2022-10-29T16:41:51.915719Z"
    },
    "papermill": {
     "duration": 0.142929,
     "end_time": "2022-10-29T16:41:51.920581",
     "exception": false,
     "start_time": "2022-10-29T16:41:51.777652",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "layout = dtensor.Layout([dtensor.UNSHARDED, dtensor.UNSHARDED], mesh_1d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97862236",
   "metadata": {
    "papermill": {
     "duration": 0.111292,
     "end_time": "2022-10-29T16:41:52.153688",
     "exception": false,
     "start_time": "2022-10-29T16:41:52.042396",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Using the same tensor and mesh the layout `Layout(['unsharded', 'x'])` would shard the second axis of the tensor across the 6 devices.\n",
    "\n",
    "<img src=\"https://www.tensorflow.org/images/dtensor/dtensor_layout_rank1.png\" alt=\"A tensor sharded across a rank-1 mesh\" class=\"no-filter\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eacd7c71",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-29T16:41:52.413712Z",
     "iopub.status.busy": "2022-10-29T16:41:52.411889Z",
     "iopub.status.idle": "2022-10-29T16:41:52.420189Z",
     "shell.execute_reply": "2022-10-29T16:41:52.419289Z"
    },
    "papermill": {
     "duration": 0.127902,
     "end_time": "2022-10-29T16:41:52.423870",
     "exception": false,
     "start_time": "2022-10-29T16:41:52.295968",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "layout = dtensor.Layout([dtensor.UNSHARDED, 'x'], mesh_1d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9821e43f",
   "metadata": {
    "papermill": {
     "duration": 0.143062,
     "end_time": "2022-10-29T16:41:52.717735",
     "exception": false,
     "start_time": "2022-10-29T16:41:52.574673",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Given a 2-dimensional 3x2 mesh such as `[(\"x\", 3), (\"y\", 2)]`, (`mesh_2d` from the previous section), `Layout([\"y\", \"x\"], mesh_2d)` is a layout for a rank-2 `Tensor` whose first axis is sharded across mesh dimension `\"y\"`, and whose second axis is sharded across mesh dimension `\"x\"`.\n",
    "\n",
    "<img src=\"https://www.tensorflow.org/images/dtensor/dtensor_layout_rank2.png\" alt=\"A tensorr with it's first axis sharded across mesh dimension 'y' and it's second axis sharded across mesh dimension 'x'\" class=\"no-filter\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "deb317c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-29T16:41:52.957814Z",
     "iopub.status.busy": "2022-10-29T16:41:52.957403Z",
     "iopub.status.idle": "2022-10-29T16:41:52.962291Z",
     "shell.execute_reply": "2022-10-29T16:41:52.961409Z"
    },
    "papermill": {
     "duration": 0.106397,
     "end_time": "2022-10-29T16:41:52.965745",
     "exception": false,
     "start_time": "2022-10-29T16:41:52.859348",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "layout = dtensor.Layout(['y', 'x'], mesh_2d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf96e77",
   "metadata": {
    "papermill": {
     "duration": 0.140093,
     "end_time": "2022-10-29T16:41:53.249333",
     "exception": false,
     "start_time": "2022-10-29T16:41:53.109240",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "For the same `mesh_2d`, the layout `Layout([\"x\", dtensor.UNSHARDED], mesh_2d)` is a layout for a rank-2 `Tensor` that is replicated across `\"y\"`, and whose first axis is sharded on mesh dimension `x`.\n",
    "\n",
    "<img src=\"https://www.tensorflow.org/images/dtensor/dtensor_layout_hybrid.png\" alt=\"A tensor replicated across mesh-dimension y, with it's first axis sharded across mesh dimension 'x'\" class=\"no-filter\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5a123ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-29T16:41:53.525894Z",
     "iopub.status.busy": "2022-10-29T16:41:53.525127Z",
     "iopub.status.idle": "2022-10-29T16:41:53.530305Z",
     "shell.execute_reply": "2022-10-29T16:41:53.529235Z"
    },
    "papermill": {
     "duration": 0.146986,
     "end_time": "2022-10-29T16:41:53.533720",
     "exception": false,
     "start_time": "2022-10-29T16:41:53.386734",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "layout = dtensor.Layout([\"x\", dtensor.UNSHARDED], mesh_2d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038a15c8",
   "metadata": {
    "papermill": {
     "duration": 0.138073,
     "end_time": "2022-10-29T16:41:53.807071",
     "exception": false,
     "start_time": "2022-10-29T16:41:53.668998",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "# <div style=\"font-family: Trebuchet MS; background-color: #D68910; color: #FFFFFF; padding: 12px; line-height: 1.5;border-radius:15px 60px 15px; text-align:center;border-color: red;border: 5px solid #17b978;overflow:hidden;\">5. | Single-Client and Multi-Client Applications ⚡</div>\n",
    "DTensor supports both single-client and multi-client applications. The kaggle Python kernel is an example of a single client DTensor application, where there is a single Python process.\n",
    "\n",
    "In a multi-client DTensor application, multiple Python processes collectively perform as a coherent application. The Cartisian grid of a `Mesh` in a multi-client DTensor application can span across devices regardless of whether they are attached locally to the current client or attached remotely to another client. The set of all devices used by a `Mesh` are called the *global device list*.\n",
    "\n",
    "The creation of a `Mesh` in a multi-client DTensor application is a collective operation where the *global device list* is identicial for all of the participating clients, and the creation of the `Mesh` serves as a global barrier.\n",
    "\n",
    "During `Mesh` creation, each client provides its *local device list* together with the expected *global device list*. DTensor validates that both lists are consistent. Please refer to the API documentation for `dtensor.create_mesh` and `dtensor.create_distributed_mesh`\n",
    " for more information on multi-client mesh creation and the *global device list*.\n",
    "\n",
    "Single-client can be thought of as a special case of multi-client, with 1 client. In a single-client application, the *global device list* is identical to the *local device list*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139898a2",
   "metadata": {
    "papermill": {
     "duration": 0.140096,
     "end_time": "2022-10-29T16:41:54.087454",
     "exception": false,
     "start_time": "2022-10-29T16:41:53.947358",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### <div style=\"font-weight:bold;line-height:1.5;padding:12px;background-color: #D68910;font-family: Trebuchet MS; color: #FFFFFF;border-radius:15px 60px 15px;border: 5px solid #17b978;text-align:center\">5.1 | DTensor as a sharded tensor</div> \n",
    "\n",
    "Now let's start coding with `DTensor`. The helper function, `dtensor_from_array`,  demonstrates creating DTensors from something that looks like a `tf.Tensor`. The function performs 2 steps:\n",
    "  - Replicates the tensor to every device on the mesh.\n",
    "  - Shards the copy according to the layout requested in its arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d509d9de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-29T16:41:54.363083Z",
     "iopub.status.busy": "2022-10-29T16:41:54.362635Z",
     "iopub.status.idle": "2022-10-29T16:41:54.369949Z",
     "shell.execute_reply": "2022-10-29T16:41:54.369060Z"
    },
    "papermill": {
     "duration": 0.147643,
     "end_time": "2022-10-29T16:41:54.373199",
     "exception": false,
     "start_time": "2022-10-29T16:41:54.225556",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def dtensor_from_array(arr, layout, shape=None, dtype=None):\n",
    "  \"\"\"Convert a DTensor from something that looks like an array or Tensor.\n",
    "\n",
    "  This function is convenient for quick doodling DTensors from a known,\n",
    "  unsharded data object in a single-client environment. This is not the\n",
    "  most efficient way of creating a DTensor, but it will do for this\n",
    "  tutorial.\n",
    "  \"\"\"\n",
    "  if shape is not None or dtype is not None:\n",
    "    arr = tf.constant(arr, shape=shape, dtype=dtype)\n",
    "\n",
    "  # replicate the input to the mesh\n",
    "  a = dtensor.copy_to_mesh(arr,\n",
    "          layout=dtensor.Layout.replicated(layout.mesh, rank=layout.rank))\n",
    "  # shard the copy to the desirable layout\n",
    "  return dtensor.relayout(a, layout=layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ca9e36",
   "metadata": {
    "papermill": {
     "duration": 0.137896,
     "end_time": "2022-10-29T16:41:54.647550",
     "exception": false,
     "start_time": "2022-10-29T16:41:54.509654",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### <div style=\"font-weight:bold;line-height:1.5;padding:12px;background-color: #D68910;font-family: Trebuchet MS; color: #FFFFFF;border-radius:15px 60px 15px;border: 5px solid #17b978;text-align:center\">5.2 | Anatomy of a DTensor</div> \n",
    "A DTensor is a `tf.Tensor` object, but augumented with the `Layout` annotation that defines its sharding behavior. A DTensor consists of the following:\n",
    "\n",
    "  - Global tensor meta-data, including the global shape and dtype of the tensor.\n",
    "  - A `Layout`, which defines the `Mesh` the `Tensor` belongs to, and how the `Tensor` is sharded onto the `Mesh`.\n",
    "  - A list of **component tensors**, one item per local device in the `Mesh`.\n",
    "\n",
    "With `dtensor_from_array`, you can create your first DTensor, `my_first_dtensor`, and examine its contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e716be8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-29T16:41:54.928753Z",
     "iopub.status.busy": "2022-10-29T16:41:54.927997Z",
     "iopub.status.idle": "2022-10-29T16:41:55.005921Z",
     "shell.execute_reply": "2022-10-29T16:41:55.004848Z"
    },
    "papermill": {
     "duration": 0.222434,
     "end_time": "2022-10-29T16:41:55.009430",
     "exception": false,
     "start_time": "2022-10-29T16:41:54.786996",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 1], layout=\"sharding_specs:unsharded, mesh:|x=6|0,1,2,3,4,5|0,1,2,3,4,5|/job:localhost/replica:0/task:0/device:CPU:0,/job:localhost/replica:0/task:0/device:CPU:1,/job:localhost/replica:0/task:0/device:CPU:2,/job:localhost/replica:0/task:0/device:CPU:3,/job:localhost/replica:0/task:0/device:CPU:4,/job:localhost/replica:0/task:0/device:CPU:5\", shape=(2,), dtype=int32)\n",
      "global shape: (2,)\n",
      "dtype: <dtype: 'int32'>\n"
     ]
    }
   ],
   "source": [
    "mesh = dtensor.create_mesh([(\"x\", 6)], devices=DEVICES)\n",
    "layout = dtensor.Layout([dtensor.UNSHARDED], mesh)\n",
    "\n",
    "my_first_dtensor = dtensor_from_array([0, 1], layout)\n",
    "\n",
    "# Examine the dtensor content\n",
    "print(my_first_dtensor)\n",
    "print(\"global shape:\", my_first_dtensor.shape)\n",
    "print(\"dtype:\", my_first_dtensor.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5cd1c5f",
   "metadata": {
    "papermill": {
     "duration": 0.137098,
     "end_time": "2022-10-29T16:41:55.287901",
     "exception": false,
     "start_time": "2022-10-29T16:41:55.150803",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "### <div style=\"font-weight:bold;line-height:1.5;padding:12px;background-color: #D68910;font-family: Trebuchet MS; color: #FFFFFF;border-radius:15px 60px 15px;border: 5px solid #17b978;text-align:center\">5.3 | Layout and `fetch_layout`</div> \n",
    "The layout of a DTensor is not a regular attribute of `tf.Tensor`. Instead, DTensor provides a function, `dtensor.fetch_layout` to access the layout of a DTensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b7d5163e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-29T16:41:55.562175Z",
     "iopub.status.busy": "2022-10-29T16:41:55.561104Z",
     "iopub.status.idle": "2022-10-29T16:41:55.570300Z",
     "shell.execute_reply": "2022-10-29T16:41:55.569298Z"
    },
    "papermill": {
     "duration": 0.15101,
     "end_time": "2022-10-29T16:41:55.573916",
     "exception": false,
     "start_time": "2022-10-29T16:41:55.422906",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sharding_specs {\n",
      "  sharding_spec: \"unsharded\"\n",
      "}\n",
      "mesh_config {\n",
      "  mesh_dimensions {\n",
      "    name: \"x\"\n",
      "    size: 6\n",
      "  }\n",
      "  global_device_ids: 0\n",
      "  global_device_ids: 1\n",
      "  global_device_ids: 2\n",
      "  global_device_ids: 3\n",
      "  global_device_ids: 4\n",
      "  global_device_ids: 5\n",
      "  local_device_ids: 0\n",
      "  local_device_ids: 1\n",
      "  local_device_ids: 2\n",
      "  local_device_ids: 3\n",
      "  local_device_ids: 4\n",
      "  local_device_ids: 5\n",
      "  local_devices: \"/job:localhost/replica:0/task:0/device:CPU:0\"\n",
      "  local_devices: \"/job:localhost/replica:0/task:0/device:CPU:1\"\n",
      "  local_devices: \"/job:localhost/replica:0/task:0/device:CPU:2\"\n",
      "  local_devices: \"/job:localhost/replica:0/task:0/device:CPU:3\"\n",
      "  local_devices: \"/job:localhost/replica:0/task:0/device:CPU:4\"\n",
      "  local_devices: \"/job:localhost/replica:0/task:0/device:CPU:5\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(dtensor.fetch_layout(my_first_dtensor))\n",
    "assert layout == dtensor.fetch_layout(my_first_dtensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4fbe84",
   "metadata": {
    "papermill": {
     "duration": 0.137742,
     "end_time": "2022-10-29T16:41:55.849707",
     "exception": false,
     "start_time": "2022-10-29T16:41:55.711965",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### <div style=\"font-weight:bold;line-height:1.5;padding:12px;background-color: #D68910;font-family: Trebuchet MS; color: #FFFFFF;border-radius:15px 60px 15px;border: 5px solid #17b978;text-align:center\">5.4 | Component tensors, `pack` and `unpack`</div> \n",
    "A DTensor consists of a list of **component tensors**. The component tensor for a device in the `Mesh` is the `Tensor` object representing the piece of the global DTensor that is stored on this device.\n",
    "\n",
    "A DTensor can be unpacked into component tensors through `dtensor.unpack`. You can make use of `dtensor.unpack` to inspect the components of the DTensor, and confirm they are on all devices of the `Mesh`.\n",
    "\n",
    "Note that the positions of component tensors in the global view may overlap each other. For example, in the case of a fully replicated layout, all components are identical replicas of the global tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8cef4cde",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-29T16:41:56.124775Z",
     "iopub.status.busy": "2022-10-29T16:41:56.123789Z",
     "iopub.status.idle": "2022-10-29T16:41:56.132611Z",
     "shell.execute_reply": "2022-10-29T16:41:56.131698Z"
    },
    "papermill": {
     "duration": 0.151547,
     "end_time": "2022-10-29T16:41:56.136084",
     "exception": false,
     "start_time": "2022-10-29T16:41:55.984537",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: /job:localhost/replica:0/task:0/device:CPU:0 , tf.Tensor([0 1], shape=(2,), dtype=int32)\n",
      "Device: /job:localhost/replica:0/task:0/device:CPU:1 , tf.Tensor([0 1], shape=(2,), dtype=int32)\n",
      "Device: /job:localhost/replica:0/task:0/device:CPU:2 , tf.Tensor([0 1], shape=(2,), dtype=int32)\n",
      "Device: /job:localhost/replica:0/task:0/device:CPU:3 , tf.Tensor([0 1], shape=(2,), dtype=int32)\n",
      "Device: /job:localhost/replica:0/task:0/device:CPU:4 , tf.Tensor([0 1], shape=(2,), dtype=int32)\n",
      "Device: /job:localhost/replica:0/task:0/device:CPU:5 , tf.Tensor([0 1], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for component_tensor in dtensor.unpack(my_first_dtensor):\n",
    "  print(\"Device:\", component_tensor.device, \",\", component_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31ccc14",
   "metadata": {
    "papermill": {
     "duration": 0.09301,
     "end_time": "2022-10-29T16:41:56.365849",
     "exception": false,
     "start_time": "2022-10-29T16:41:56.272839",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "As shown, `my_first_dtensor` is a tensor of `[0, 1]` replicated to all 6 devices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1563d8",
   "metadata": {
    "papermill": {
     "duration": 0.067437,
     "end_time": "2022-10-29T16:41:56.500909",
     "exception": false,
     "start_time": "2022-10-29T16:41:56.433472",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The inverse operation of `dtensor.unpack` is `dtensor.pack`. Component tensors can be packed back into a DTensor.\n",
    "\n",
    "The components must have the same rank and dtype, which will be the rank and dtype of the returned DTensor. However there is no strict requirement on the device placement of component tensors as inputs of `dtensor.unpack`: the function will automatically copy the component tensors to their respective corresponding devices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "40854ec3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-29T16:41:56.638407Z",
     "iopub.status.busy": "2022-10-29T16:41:56.637942Z",
     "iopub.status.idle": "2022-10-29T16:41:56.645797Z",
     "shell.execute_reply": "2022-10-29T16:41:56.644443Z"
    },
    "papermill": {
     "duration": 0.079856,
     "end_time": "2022-10-29T16:41:56.648456",
     "exception": false,
     "start_time": "2022-10-29T16:41:56.568600",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 1], layout=\"sharding_specs:unsharded, mesh:|x=6|0,1,2,3,4,5|0,1,2,3,4,5|/job:localhost/replica:0/task:0/device:CPU:0,/job:localhost/replica:0/task:0/device:CPU:1,/job:localhost/replica:0/task:0/device:CPU:2,/job:localhost/replica:0/task:0/device:CPU:3,/job:localhost/replica:0/task:0/device:CPU:4,/job:localhost/replica:0/task:0/device:CPU:5\", shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "packed_dtensor = dtensor.pack(\n",
    "    [[0, 1], [0, 1], [0, 1],\n",
    "     [0, 1], [0, 1], [0, 1]],\n",
    "     layout=layout\n",
    ")\n",
    "print(packed_dtensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39acd273",
   "metadata": {
    "papermill": {
     "duration": 0.067173,
     "end_time": "2022-10-29T16:41:56.785600",
     "exception": false,
     "start_time": "2022-10-29T16:41:56.718427",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <div style=\"font-family: Trebuchet MS; background-color: #D68910; color: #FFFFFF; padding: 12px; line-height: 1.5;border-radius:15px 60px 15px; text-align:center;border-color: red;border: 5px solid #17b978;overflow:hidden;\">6. | Sharding a DTensor to a Mesh</div>\n",
    "\n",
    "So far you've worked with the `my_first_dtensor`, which is a rank-1 DTensor fully replicated across a dim-1 `Mesh`.\n",
    "\n",
    "Next create and inspect DTensors that are sharded across a dim-2 `Mesh`. The next example does this with a 3x2 `Mesh` on 6 CPU devices, where size of mesh dimension `'x'` is 3 devices, and size of mesh dimension`'y'` is 2 devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "11ce3a9e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-29T16:41:56.926097Z",
     "iopub.status.busy": "2022-10-29T16:41:56.925329Z",
     "iopub.status.idle": "2022-10-29T16:41:56.930979Z",
     "shell.execute_reply": "2022-10-29T16:41:56.930015Z"
    },
    "papermill": {
     "duration": 0.079525,
     "end_time": "2022-10-29T16:41:56.933433",
     "exception": false,
     "start_time": "2022-10-29T16:41:56.853908",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "mesh = dtensor.create_mesh([(\"x\", 3), (\"y\", 2)], devices=DEVICES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd004fd",
   "metadata": {
    "papermill": {
     "duration": 0.067988,
     "end_time": "2022-10-29T16:41:57.071030",
     "exception": false,
     "start_time": "2022-10-29T16:41:57.003042",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### <div style=\"font-weight:bold;line-height:1.5;padding:12px;background-color: #D68910;font-family: Trebuchet MS; color: #FFFFFF;border-radius:15px 60px 15px;border: 5px solid #17b978;text-align:center\">6.1 | Fully sharded rank-2 Tensor on a dim-2 Mesh</div> \n",
    "Create a 3x2 rank-2 DTensor, sharding its first axis along the `'x'` mesh dimension, and its second axis along the `'y'` mesh dimension.\n",
    "\n",
    "- Because the tensor shape equals to the mesh dimension along all of the sharded axes, each device receives a single element of the DTensor.\n",
    "- The rank of the component tensor is always the same as the rank of the global shape. DTensor adopts this convention as a simple way to preserve information for locating the relation between a component tensor and the global DTensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6a385bb0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-29T16:41:57.209067Z",
     "iopub.status.busy": "2022-10-29T16:41:57.208345Z",
     "iopub.status.idle": "2022-10-29T16:41:57.251540Z",
     "shell.execute_reply": "2022-10-29T16:41:57.250388Z"
    },
    "papermill": {
     "duration": 0.114857,
     "end_time": "2022-10-29T16:41:57.254194",
     "exception": false,
     "start_time": "2022-10-29T16:41:57.139337",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: /job:localhost/replica:0/task:0/device:CPU:0 , tf.Tensor([[0]], shape=(1, 1), dtype=int32)\n",
      "Device: /job:localhost/replica:0/task:0/device:CPU:1 , tf.Tensor([[1]], shape=(1, 1), dtype=int32)\n",
      "Device: /job:localhost/replica:0/task:0/device:CPU:2 , tf.Tensor([[2]], shape=(1, 1), dtype=int32)\n",
      "Device: /job:localhost/replica:0/task:0/device:CPU:3 , tf.Tensor([[3]], shape=(1, 1), dtype=int32)\n",
      "Device: /job:localhost/replica:0/task:0/device:CPU:4 , tf.Tensor([[4]], shape=(1, 1), dtype=int32)\n",
      "Device: /job:localhost/replica:0/task:0/device:CPU:5 , tf.Tensor([[5]], shape=(1, 1), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "fully_sharded_dtensor = dtensor_from_array(\n",
    "    tf.reshape(tf.range(6), (3, 2)),\n",
    "    layout=dtensor.Layout([\"x\", \"y\"], mesh))\n",
    "\n",
    "for raw_component in dtensor.unpack(fully_sharded_dtensor):\n",
    "  print(\"Device:\", raw_component.device, \",\", raw_component)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bd5d10",
   "metadata": {
    "papermill": {
     "duration": 0.068597,
     "end_time": "2022-10-29T16:41:57.394226",
     "exception": false,
     "start_time": "2022-10-29T16:41:57.325629",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### <div style=\"font-weight:bold;line-height:1.5;padding:12px;background-color: #D68910;font-family: Trebuchet MS; color: #FFFFFF;border-radius:15px 60px 15px;border: 5px solid #17b978;text-align:center\">6.2 | Fully replicated rank-2 Tensor on a dim-2 Mesh</div> \n",
    "For comparison, create a 3x2 rank-2 DTensor, fully replicated to the same dim-2 Mesh.\n",
    "\n",
    " - Because the DTensor is fully replicated, each device receives a full replica of the 3x2 DTensor.\n",
    " - The rank of the component tensors are the same as the rank of the global shape -- this fact is trivial, because in this case, the shape of the component tensors are the same as the global shape anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ed7b1bd5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-29T16:41:57.532595Z",
     "iopub.status.busy": "2022-10-29T16:41:57.532218Z",
     "iopub.status.idle": "2022-10-29T16:41:57.552772Z",
     "shell.execute_reply": "2022-10-29T16:41:57.551413Z"
    },
    "papermill": {
     "duration": 0.092543,
     "end_time": "2022-10-29T16:41:57.555518",
     "exception": false,
     "start_time": "2022-10-29T16:41:57.462975",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: /job:localhost/replica:0/task:0/device:CPU:0 , tf.Tensor(\n",
      "[[0 1]\n",
      " [2 3]\n",
      " [4 5]], shape=(3, 2), dtype=int32)\n",
      "Device: /job:localhost/replica:0/task:0/device:CPU:1 , tf.Tensor(\n",
      "[[0 1]\n",
      " [2 3]\n",
      " [4 5]], shape=(3, 2), dtype=int32)\n",
      "Device: /job:localhost/replica:0/task:0/device:CPU:2 , tf.Tensor(\n",
      "[[0 1]\n",
      " [2 3]\n",
      " [4 5]], shape=(3, 2), dtype=int32)\n",
      "Device: /job:localhost/replica:0/task:0/device:CPU:3 , tf.Tensor(\n",
      "[[0 1]\n",
      " [2 3]\n",
      " [4 5]], shape=(3, 2), dtype=int32)\n",
      "Device: /job:localhost/replica:0/task:0/device:CPU:4 , tf.Tensor(\n",
      "[[0 1]\n",
      " [2 3]\n",
      " [4 5]], shape=(3, 2), dtype=int32)\n",
      "Device: /job:localhost/replica:0/task:0/device:CPU:5 , tf.Tensor(\n",
      "[[0 1]\n",
      " [2 3]\n",
      " [4 5]], shape=(3, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "fully_replicated_dtensor = dtensor_from_array(\n",
    "    tf.reshape(tf.range(6), (3, 2)),\n",
    "    layout=dtensor.Layout([dtensor.UNSHARDED, dtensor.UNSHARDED], mesh))\n",
    "# Or, layout=tensor.Layout.fully_replicated(mesh, rank=2)\n",
    "\n",
    "for component_tensor in dtensor.unpack(fully_replicated_dtensor):\n",
    "  print(\"Device:\", component_tensor.device, \",\", component_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e500b798",
   "metadata": {
    "papermill": {
     "duration": 0.070072,
     "end_time": "2022-10-29T16:41:57.695154",
     "exception": false,
     "start_time": "2022-10-29T16:41:57.625082",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### <div style=\"font-weight:bold;line-height:1.5;padding:12px;background-color: #D68910;font-family: Trebuchet MS; color: #FFFFFF;border-radius:15px 60px 15px;border: 5px solid #17b978;text-align:center\">6.3 | Hybrid rank-2 Tensor on a dim-2 Mesh</div> \n",
    "What about somewhere between fully sharded and fully replicated?\n",
    "\n",
    "DTensor allows a `Layout` to be a hybrid, sharded along some axes, but replicated along others.\n",
    "\n",
    "For example, you can shard the same 3x2 rank-2 DTensor in the following way:\n",
    "\n",
    "  - 1st axis sharded along the `'x'` mesh dimension.\n",
    "  - 2nd axis replicated along the `'y'` mesh dimension.\n",
    "\n",
    "To achieve this sharding scheme, you just need to replace the sharding spec of the 2nd axis from `'y'` to `dtensor.UNSHARDED`, to indicate your intention of replicating along the 2nd axis. The layout object will look like `Layout(['x', dtensor.UNSHARDED], mesh)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a5ab74c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-29T16:41:57.836656Z",
     "iopub.status.busy": "2022-10-29T16:41:57.836227Z",
     "iopub.status.idle": "2022-10-29T16:41:57.863722Z",
     "shell.execute_reply": "2022-10-29T16:41:57.862384Z"
    },
    "papermill": {
     "duration": 0.101517,
     "end_time": "2022-10-29T16:41:57.866221",
     "exception": false,
     "start_time": "2022-10-29T16:41:57.764704",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: /job:localhost/replica:0/task:0/device:CPU:0 , tf.Tensor([[0 1]], shape=(1, 2), dtype=int32)\n",
      "Device: /job:localhost/replica:0/task:0/device:CPU:1 , tf.Tensor([[0 1]], shape=(1, 2), dtype=int32)\n",
      "Device: /job:localhost/replica:0/task:0/device:CPU:2 , tf.Tensor([[2 3]], shape=(1, 2), dtype=int32)\n",
      "Device: /job:localhost/replica:0/task:0/device:CPU:3 , tf.Tensor([[2 3]], shape=(1, 2), dtype=int32)\n",
      "Device: /job:localhost/replica:0/task:0/device:CPU:4 , tf.Tensor([[4 5]], shape=(1, 2), dtype=int32)\n",
      "Device: /job:localhost/replica:0/task:0/device:CPU:5 , tf.Tensor([[4 5]], shape=(1, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "hybrid_sharded_dtensor = dtensor_from_array(\n",
    "    tf.reshape(tf.range(6), (3, 2)),\n",
    "    layout=dtensor.Layout(['x', dtensor.UNSHARDED], mesh))\n",
    "\n",
    "for component_tensor in dtensor.unpack(hybrid_sharded_dtensor):\n",
    "  print(\"Device:\", component_tensor.device, \",\", component_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3de7ae6",
   "metadata": {
    "papermill": {
     "duration": 0.067597,
     "end_time": "2022-10-29T16:41:58.003573",
     "exception": false,
     "start_time": "2022-10-29T16:41:57.935976",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "You can inspect the component tensors of the created DTensor and verify they are indeed sharded according to your scheme. It may be helpful to illustrate the situation with a chart:\n",
    "\n",
    "  <img src=\"https://www.tensorflow.org/images/dtensor/dtensor_hybrid_mesh.png\" alt=\"A 3x2 hybrid mesh with 6 CPUs\"\n",
    "     class=\"no-filter\" width=75%>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4afa0b7",
   "metadata": {
    "papermill": {
     "duration": 0.067573,
     "end_time": "2022-10-29T16:41:58.139364",
     "exception": false,
     "start_time": "2022-10-29T16:41:58.071791",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### <div style=\"font-weight:bold;line-height:1.5;padding:12px;background-color: #D68910;font-family: Trebuchet MS; color: #FFFFFF;border-radius:15px 60px 15px;border: 5px solid #17b978;text-align:center\">6.4 | Tensor.numpy() and sharded DTensor</div>\n",
    "\n",
    "Be aware that calling the `.numpy()` method on a sharded DTensor raises an error. The rationale for erroring is to protect against unintended gathering of data from multiple computing devices to the host CPU device backing the returned numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9b3af145",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-29T16:41:58.277307Z",
     "iopub.status.busy": "2022-10-29T16:41:58.276529Z",
     "iopub.status.idle": "2022-10-29T16:41:58.284511Z",
     "shell.execute_reply": "2022-10-29T16:41:58.283177Z"
    },
    "papermill": {
     "duration": 0.079591,
     "end_time": "2022-10-29T16:41:58.286742",
     "exception": false,
     "start_time": "2022-10-29T16:41:58.207151",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1]\n",
      " [2 3]\n",
      " [4 5]]\n",
      "got an error as expected for fully_sharded_dtensor\n",
      "got an error as expected for hybrid_sharded_dtensor\n"
     ]
    }
   ],
   "source": [
    "print(fully_replicated_dtensor.numpy())\n",
    "\n",
    "try:\n",
    "  fully_sharded_dtensor.numpy()\n",
    "except tf.errors.UnimplementedError:\n",
    "  print(\"got an error as expected for fully_sharded_dtensor\")\n",
    "\n",
    "try:\n",
    "  hybrid_sharded_dtensor.numpy()\n",
    "except tf.errors.UnimplementedError:\n",
    "  print(\"got an error as expected for hybrid_sharded_dtensor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9b6a72",
   "metadata": {
    "papermill": {
     "duration": 0.067579,
     "end_time": "2022-10-29T16:41:58.421862",
     "exception": false,
     "start_time": "2022-10-29T16:41:58.354283",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <div style=\"font-family: Trebuchet MS; background-color: #D68910; color: #FFFFFF; padding: 12px; line-height: 1.5;border-radius:15px 60px 15px; text-align:center;border-color: red;border: 5px solid #17b978;overflow:hidden;\">7. | TensorFlow API on DTensor</div>\n",
    "DTensor strives to be a drop-in replacement for tensor in your program. The TensorFlow Python API that consume `tf.Tensor`, such as the Ops library functions, `tf.function`, `tf.GradientTape`, also work with DTensor.\n",
    "\n",
    "To accomplish this, for each [TensorFlow Graph](https://www.tensorflow.org/guide/intro_to_graphs), DTensor produces and executes an equivalent [SPMD](https://en.wikipedia.org/wiki/SPMD) graph in a procedure called *SPMD expansion*. A few critical steps in DTensor SPMD expansion are:\n",
    "\n",
    "  - Propagating the sharding `Layout` of DTensor in the TensorFlow graph\n",
    "  - Rewriting TensorFlow Ops on the global DTensor with equivalent TensorFlow Ops on the component tensors, inserting collective and communication Ops when necessary\n",
    "  - Lowering backend neutral TensorFlow Ops to backend specific TensorFlow Ops.\n",
    "\n",
    "The final result is that **DTensor is a drop-in replacement for Tensor**.\n",
    "\n",
    "Note: DTensor is still an experimental API which means you will be exploring and pushing the boundaries and limits of the DTensor programming model.\n",
    "\n",
    "There are 2 ways of triggering DTensor execution:\n",
    "  - DTensor as operands of a Python function, e.g. `tf.matmul(a, b)` will run through DTensor if `a`, `b`, or both are DTensors.\n",
    "  - Requesting the result of a Python function to be a DTensor, e.g. `dtensor.call_with_layout(tf.ones, layout, shape=(3, 2))` will run through DTensor because we requested the output of tf.ones to be sharded according to a `layout`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d0d425",
   "metadata": {
    "papermill": {
     "duration": 0.067968,
     "end_time": "2022-10-29T16:41:58.557764",
     "exception": false,
     "start_time": "2022-10-29T16:41:58.489796",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### <div style=\"font-weight:bold;line-height:1.5;padding:12px;background-color: #D68910;font-family: Trebuchet MS; color: #FFFFFF;border-radius:15px 60px 15px;border: 5px solid #17b978;text-align:center\">7.1 | DTensor as Operands</div>\n",
    "\n",
    "Many TensorFlow API functions take `tf.Tensor` as their operands, and returns `tf.Tensor` as their results. For these functions, you can express intention to run a function through DTensor by passing in DTensor as operands. This section uses `tf.matmul(a, b)` as an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504fa391",
   "metadata": {
    "papermill": {
     "duration": 0.068769,
     "end_time": "2022-10-29T16:41:58.694422",
     "exception": false,
     "start_time": "2022-10-29T16:41:58.625653",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### <div style=\"font-weight:bold;line-height:1.5;padding:12px;background-color: #D68910;font-family: Trebuchet MS; color: #FFFFFF;border-radius:15px 60px 15px;border: 5px solid #17b978;text-align:center\">7.2 | Fully replicated input and output</div>\n",
    "\n",
    "In this case, the DTensors are fully replicated. On each of the devices of the `Mesh`,\n",
    "  - the component tensor for operand `a` is `[[1, 2, 3], [4, 5, 6]]` (2x3)\n",
    "  - the component tensor for operand `b` is `[[6, 5], [4, 3], [2, 1]]` (3x2)\n",
    "  - the computation consists of a single `MatMul` of `(2x3, 3x2) -> 2x2`,\n",
    "  - the component tensor for result `c` is `[[20, 14], [56,41]]` (2x2)\n",
    "\n",
    "Total number of floating point mul operations is `6 device * 4 result * 3 mul = 72`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f4524ad5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-29T16:41:58.833794Z",
     "iopub.status.busy": "2022-10-29T16:41:58.833050Z",
     "iopub.status.idle": "2022-10-29T16:41:58.882860Z",
     "shell.execute_reply": "2022-10-29T16:41:58.882085Z"
    },
    "papermill": {
     "duration": 0.12221,
     "end_time": "2022-10-29T16:41:58.885414",
     "exception": false,
     "start_time": "2022-10-29T16:41:58.763204",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sharding spec: ['unsharded', 'unsharded']\n",
      "components:\n",
      "/job:localhost/replica:0/task:0/device:CPU:0 [[20 14]\n",
      " [56 41]]\n",
      "/job:localhost/replica:0/task:0/device:CPU:1 [[20 14]\n",
      " [56 41]]\n",
      "/job:localhost/replica:0/task:0/device:CPU:2 [[20 14]\n",
      " [56 41]]\n",
      "/job:localhost/replica:0/task:0/device:CPU:3 [[20 14]\n",
      " [56 41]]\n",
      "/job:localhost/replica:0/task:0/device:CPU:4 [[20 14]\n",
      " [56 41]]\n",
      "/job:localhost/replica:0/task:0/device:CPU:5 [[20 14]\n",
      " [56 41]]\n"
     ]
    }
   ],
   "source": [
    "mesh = dtensor.create_mesh([(\"x\", 6)], devices=DEVICES)\n",
    "layout = dtensor.Layout([dtensor.UNSHARDED, dtensor.UNSHARDED], mesh)\n",
    "a = dtensor_from_array([[1, 2, 3], [4, 5, 6]], layout=layout)\n",
    "b = dtensor_from_array([[6, 5], [4, 3], [2, 1]], layout=layout)\n",
    "\n",
    "c = tf.matmul(a, b) # runs 6 identical matmuls in parallel on 6 devices\n",
    "\n",
    "# `c` is a DTensor replicated on all devices (same as `a` and `b`)\n",
    "print('Sharding spec:', dtensor.fetch_layout(c).sharding_specs)\n",
    "print(\"components:\")\n",
    "for component_tensor in dtensor.unpack(c):\n",
    "  print(component_tensor.device, component_tensor.numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401213cd",
   "metadata": {
    "papermill": {
     "duration": 0.067379,
     "end_time": "2022-10-29T16:41:59.022672",
     "exception": false,
     "start_time": "2022-10-29T16:41:58.955293",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "### <div style=\"font-weight:bold;line-height:1.5;padding:12px;background-color: #D68910;font-family: Trebuchet MS; color: #FFFFFF;border-radius:15px 60px 15px;border: 5px solid #17b978;text-align:center\">7.3 | Sharding operands along the contracted axis</div>\n",
    "\n",
    "You can reduce the amount of computation per device by sharding the operands `a` and `b`. A popular sharding scheme for `tf.matmul` is to shard the operands along the axis of the contraction, which means sharding `a` along the second axis, and `b` along the first axis.\n",
    "\n",
    "The global matrix product sharded under this scheme can be performed efficiently, by local matmuls that runs concurrently, followed by a collective reduction to aggregate the local results. This is also the [canonical way](https://github.com/open-mpi/ompi/blob/ee87ec391f48512d3718fc7c8b13596403a09056/docs/man-openmpi/man3/MPI_Reduce.3.rst?plain=1#L265) of implementing a distributed matrix dot product.\n",
    "\n",
    "Total number of floating point mul operations is `6 devices * 4 result * 1 = 24`, a factor of 3 reduction compared to the fully replicated case (72) above. The factor of 3 is due to the sharing along `x` mesh dimension with a size of `3` devices.\n",
    "\n",
    "The reduction of the number of operations run sequentially is the main mechansism with which synchronuous model parallelism accelerates training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cdc15f45",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-29T16:41:59.161348Z",
     "iopub.status.busy": "2022-10-29T16:41:59.160035Z",
     "iopub.status.idle": "2022-10-29T16:41:59.227715Z",
     "shell.execute_reply": "2022-10-29T16:41:59.226837Z"
    },
    "papermill": {
     "duration": 0.139227,
     "end_time": "2022-10-29T16:41:59.229967",
     "exception": false,
     "start_time": "2022-10-29T16:41:59.090740",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sharding spec: ['unsharded', 'unsharded']\n",
      "/job:localhost/replica:0/task:0/device:CPU:0 [[20 14]\n",
      " [56 41]]\n",
      "/job:localhost/replica:0/task:0/device:CPU:1 [[20 14]\n",
      " [56 41]]\n",
      "/job:localhost/replica:0/task:0/device:CPU:2 [[20 14]\n",
      " [56 41]]\n",
      "/job:localhost/replica:0/task:0/device:CPU:3 [[20 14]\n",
      " [56 41]]\n",
      "/job:localhost/replica:0/task:0/device:CPU:4 [[20 14]\n",
      " [56 41]]\n",
      "/job:localhost/replica:0/task:0/device:CPU:5 [[20 14]\n",
      " [56 41]]\n"
     ]
    }
   ],
   "source": [
    "mesh = dtensor.create_mesh([(\"x\", 3), (\"y\", 2)], devices=DEVICES)\n",
    "a_layout = dtensor.Layout([dtensor.UNSHARDED, 'x'], mesh)\n",
    "a = dtensor_from_array([[1, 2, 3], [4, 5, 6]], layout=a_layout)\n",
    "b_layout = dtensor.Layout(['x', dtensor.UNSHARDED], mesh)\n",
    "b = dtensor_from_array([[6, 5], [4, 3], [2, 1]], layout=b_layout)\n",
    "\n",
    "c = tf.matmul(a, b)\n",
    "# `c` is a DTensor replicated on all devices (same as `a` and `b`)\n",
    "print('Sharding spec:', dtensor.fetch_layout(c).sharding_specs)\n",
    "for component_tensor in dtensor.unpack(c):\n",
    "  print(component_tensor.device, component_tensor.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69c97f0",
   "metadata": {
    "papermill": {
     "duration": 0.068104,
     "end_time": "2022-10-29T16:41:59.367674",
     "exception": false,
     "start_time": "2022-10-29T16:41:59.299570",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### <div style=\"font-weight:bold;line-height:1.5;padding:12px;background-color: #D68910;font-family: Trebuchet MS; color: #FFFFFF;border-radius:15px 60px 15px;border: 5px solid #17b978;text-align:center\">7.4 | Additional Sharding</div>\n",
    "\n",
    "You can perform additional sharding on the inputs, and they are appropriately carried over to the results. For example, you can apply additional sharding of operand `a` along its first axis to the `'y'` mesh dimension. The additional sharding will be carried over to the first axis of the result `c`.\n",
    "\n",
    "\n",
    "Total number of floating point mul operations is `6 devices * 2 result * 1 = 12`, an additional factor of 2 reduction compared to the case (24) above. The factor of 2 is due to the sharing along `y` mesh dimension with a size of `2` devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f033e92a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-29T16:41:59.506691Z",
     "iopub.status.busy": "2022-10-29T16:41:59.505429Z",
     "iopub.status.idle": "2022-10-29T16:41:59.562638Z",
     "shell.execute_reply": "2022-10-29T16:41:59.561264Z"
    },
    "papermill": {
     "duration": 0.129259,
     "end_time": "2022-10-29T16:41:59.565160",
     "exception": false,
     "start_time": "2022-10-29T16:41:59.435901",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sharding spec: ['y', 'unsharded']\n",
      "components:\n",
      "/job:localhost/replica:0/task:0/device:CPU:0 [[20 14]]\n",
      "/job:localhost/replica:0/task:0/device:CPU:1 [[56 41]]\n",
      "/job:localhost/replica:0/task:0/device:CPU:2 [[20 14]]\n",
      "/job:localhost/replica:0/task:0/device:CPU:3 [[56 41]]\n",
      "/job:localhost/replica:0/task:0/device:CPU:4 [[20 14]]\n",
      "/job:localhost/replica:0/task:0/device:CPU:5 [[56 41]]\n"
     ]
    }
   ],
   "source": [
    "mesh = dtensor.create_mesh([(\"x\", 3), (\"y\", 2)], devices=DEVICES)\n",
    "\n",
    "a_layout = dtensor.Layout(['y', 'x'], mesh)\n",
    "a = dtensor_from_array([[1, 2, 3], [4, 5, 6]], layout=a_layout)\n",
    "b_layout = dtensor.Layout(['x', dtensor.UNSHARDED], mesh)\n",
    "b = dtensor_from_array([[6, 5], [4, 3], [2, 1]], layout=b_layout)\n",
    "\n",
    "c = tf.matmul(a, b)\n",
    "# The sharding of `a` on the first axis is carried to `c'\n",
    "print('Sharding spec:', dtensor.fetch_layout(c).sharding_specs)\n",
    "print(\"components:\")\n",
    "for component_tensor in dtensor.unpack(c):\n",
    "  print(component_tensor.device, component_tensor.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3c000c",
   "metadata": {
    "papermill": {
     "duration": 0.069546,
     "end_time": "2022-10-29T16:41:59.706464",
     "exception": false,
     "start_time": "2022-10-29T16:41:59.636918",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "# <div style=\"font-weight:bold;line-height:1.5;padding:12px;background-color: #D68910;font-family: Trebuchet MS; color: #FFFFFF;border-radius:15px 60px 15px;border: 5px solid #17b978;text-align:center\">8. | DTensor as Output</div>\n",
    "What about Python functions that do not take operands, but returns a Tensor result that can be sharded? Examples of such functions are\n",
    "\n",
    "  - `tf.ones`, `tf.zeros`, `tf.random.stateless_normal`,\n",
    "\n",
    "For these Python functions, DTensor provides `dtensor.call_with_layout` which eagerly executes a Python function with DTensor, and ensures that the returned Tensor is a DTensor with the requested `Layout`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8f7b3353",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-29T16:41:59.850471Z",
     "iopub.status.busy": "2022-10-29T16:41:59.850029Z",
     "iopub.status.idle": "2022-10-29T16:41:59.856475Z",
     "shell.execute_reply": "2022-10-29T16:41:59.855146Z"
    },
    "papermill": {
     "duration": 0.081155,
     "end_time": "2022-10-29T16:41:59.859446",
     "exception": false,
     "start_time": "2022-10-29T16:41:59.778291",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function call_with_layout in module tensorflow.dtensor.python.api:\n",
      "\n",
      "call_with_layout(fn: Callable[..., Any], layout: Union[tensorflow.dtensor.python.layout.Layout, NoneType], *args, **kwargs) -> Any\n",
      "    Calls a function in the DTensor device scope if `layout` is not None.\n",
      "    \n",
      "    If `layout` is not None, `fn` consumes DTensor(s) as input and produces a\n",
      "    DTensor as output; a DTensor is a tf.Tensor with layout-related attributes.\n",
      "    \n",
      "    If `layout` is None, `fn` consumes and produces regular tf.Tensors.\n",
      "    \n",
      "    Args:\n",
      "      fn: A supported TF API function such as tf.zeros.\n",
      "      layout: Optional, the layout of the output DTensor.\n",
      "      *args:  Arguments given to `fn`.\n",
      "      **kwargs: Keyword arguments given to `fn`.\n",
      "    \n",
      "    Returns:\n",
      "      The return value of `fn` transformed to a DTensor if requested.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(dtensor.call_with_layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8bfe46",
   "metadata": {
    "papermill": {
     "duration": 0.069452,
     "end_time": "2022-10-29T16:41:59.997505",
     "exception": false,
     "start_time": "2022-10-29T16:41:59.928053",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The eagerly executed Python function usually only contain a single non-trivial TensorFlow Op.\n",
    "\n",
    "To use a Python function that emits multiple TensorFlow Ops with `dtensor.call_with_layout`, the function should be converted to a `tf.function`. Calling a `tf.function` is a single TensorFlow Op. When the `tf.function` is called, DTensor can perform layout propagation when it analyzes the computing graph of the `tf.function`, before any of the intermediate tensors are materialized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11324b6a",
   "metadata": {
    "papermill": {
     "duration": 0.068708,
     "end_time": "2022-10-29T16:42:00.135006",
     "exception": false,
     "start_time": "2022-10-29T16:42:00.066298",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### <div style=\"font-weight:bold;line-height:1.5;padding:12px;background-color: #D68910;font-family: Trebuchet MS; color: #FFFFFF;border-radius:15px 60px 15px;border: 5px solid #17b978;text-align:center\">8.1 | APIs that emit a single TensorFlow Op</div>\n",
    "\n",
    "If a function  emits a single TensorFlow Op, you can directly apply `dtensor.call_with_layout` to the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1d8df55b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-29T16:42:00.283004Z",
     "iopub.status.busy": "2022-10-29T16:42:00.282629Z",
     "iopub.status.idle": "2022-10-29T16:42:00.287634Z",
     "shell.execute_reply": "2022-10-29T16:42:00.286851Z"
    },
    "papermill": {
     "duration": 0.085928,
     "end_time": "2022-10-29T16:42:00.290259",
     "exception": false,
     "start_time": "2022-10-29T16:42:00.204331",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function ones in module tensorflow.python.ops.array_ops:\n",
      "\n",
      "ones(shape, dtype=tf.float32, name=None)\n",
      "    Creates a tensor with all elements set to one (1).\n",
      "    \n",
      "    See also `tf.ones_like`, `tf.zeros`, `tf.fill`, `tf.eye`.\n",
      "    \n",
      "    This operation returns a tensor of type `dtype` with shape `shape` and\n",
      "    all elements set to one.\n",
      "    \n",
      "    >>> tf.ones([3, 4], tf.int32)\n",
      "    <tf.Tensor: shape=(3, 4), dtype=int32, numpy=\n",
      "    array([[1, 1, 1, 1],\n",
      "           [1, 1, 1, 1],\n",
      "           [1, 1, 1, 1]], dtype=int32)>\n",
      "    \n",
      "    Args:\n",
      "      shape: A `list` of integers, a `tuple` of integers, or\n",
      "        a 1-D `Tensor` of type `int32`.\n",
      "      dtype: Optional DType of an element in the resulting `Tensor`. Default is\n",
      "        `tf.float32`.\n",
      "      name: Optional string. A name for the operation.\n",
      "    \n",
      "    Returns:\n",
      "      A `Tensor` with all elements set to one (1).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ae66bfed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-29T16:42:00.431027Z",
     "iopub.status.busy": "2022-10-29T16:42:00.430627Z",
     "iopub.status.idle": "2022-10-29T16:42:00.452242Z",
     "shell.execute_reply": "2022-10-29T16:42:00.450770Z"
    },
    "papermill": {
     "duration": 0.094348,
     "end_time": "2022-10-29T16:42:00.454673",
     "exception": false,
     "start_time": "2022-10-29T16:42:00.360325",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor({\"CPU:0\": [[1 1]\n",
      " [1 1]], \"CPU:1\": [[1 1]\n",
      " [1 1]], \"CPU:2\": [[1 1]\n",
      " [1 1]], \"CPU:3\": [[1 1]\n",
      " [1 1]], \"CPU:4\": [[1 1]\n",
      " [1 1]], \"CPU:5\": [[1 1]\n",
      " [1 1]]}, layout=\"sharding_specs:x,y, mesh:|x=3,y=2|0,1,2,3,4,5|0,1,2,3,4,5|/job:localhost/replica:0/task:0/device:CPU:0,/job:localhost/replica:0/task:0/device:CPU:1,/job:localhost/replica:0/task:0/device:CPU:2,/job:localhost/replica:0/task:0/device:CPU:3,/job:localhost/replica:0/task:0/device:CPU:4,/job:localhost/replica:0/task:0/device:CPU:5\", shape=(6, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "mesh = dtensor.create_mesh([(\"x\", 3), (\"y\", 2)], devices=DEVICES)\n",
    "ones = dtensor.call_with_layout(tf.ones, dtensor.Layout(['x', 'y'], mesh), shape=(6, 4))\n",
    "print(ones)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c700f59f",
   "metadata": {
    "papermill": {
     "duration": 0.06876,
     "end_time": "2022-10-29T16:42:00.592748",
     "exception": false,
     "start_time": "2022-10-29T16:42:00.523988",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "#### <div style=\"font-weight:bold;line-height:1.5;padding:12px;background-color: #D68910;font-family: Trebuchet MS; color: #FFFFFF;border-radius:15px 60px 15px;border: 5px solid #17b978;text-align:center\">8.2 | APIs that emit multiple TensorFlow Ops</div>\n",
    "\n",
    "If the API emits multiple TensorFlow Ops, convert the function into a single Op through `tf.function`. For example `tf.random.stateleess_normal`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "363b50a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-29T16:42:00.735458Z",
     "iopub.status.busy": "2022-10-29T16:42:00.734645Z",
     "iopub.status.idle": "2022-10-29T16:42:00.740636Z",
     "shell.execute_reply": "2022-10-29T16:42:00.739448Z"
    },
    "papermill": {
     "duration": 0.081869,
     "end_time": "2022-10-29T16:42:00.744211",
     "exception": false,
     "start_time": "2022-10-29T16:42:00.662342",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function stateless_random_normal in module tensorflow.python.ops.stateless_random_ops:\n",
      "\n",
      "stateless_random_normal(shape, seed, mean=0.0, stddev=1.0, dtype=tf.float32, name=None, alg='auto_select')\n",
      "    Outputs deterministic pseudorandom values from a normal distribution.\n",
      "    \n",
      "    This is a stateless version of `tf.random.normal`: if run twice with the\n",
      "    same seeds and shapes, it will produce the same pseudorandom numbers.  The\n",
      "    output is consistent across multiple runs on the same hardware (and between\n",
      "    CPU and GPU), but may change between versions of TensorFlow or on non-CPU/GPU\n",
      "    hardware.\n",
      "    \n",
      "    Args:\n",
      "      shape: A 1-D integer Tensor or Python array. The shape of the output tensor.\n",
      "      seed: A shape [2] Tensor, the seed to the random number generator. Must have\n",
      "        dtype `int32` or `int64`. (When using XLA, only `int32` is allowed.)\n",
      "      mean: A 0-D Tensor or Python value of type `dtype`. The mean of the normal\n",
      "        distribution.\n",
      "      stddev: A 0-D Tensor or Python value of type `dtype`. The standard deviation\n",
      "        of the normal distribution.\n",
      "      dtype: The float type of the output: `float16`, `bfloat16`, `float32`,\n",
      "        `float64`. Defaults to `float32`.\n",
      "      name: A name for the operation (optional).\n",
      "      alg: The RNG algorithm used to generate the random numbers. See\n",
      "        `tf.random.stateless_uniform` for a detailed explanation.\n",
      "    \n",
      "    Returns:\n",
      "      A tensor of the specified shape filled with random normal values.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.random.stateless_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d746dad2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-29T16:42:00.884072Z",
     "iopub.status.busy": "2022-10-29T16:42:00.883644Z",
     "iopub.status.idle": "2022-10-29T16:42:00.967191Z",
     "shell.execute_reply": "2022-10-29T16:42:00.965658Z"
    },
    "papermill": {
     "duration": 0.156643,
     "end_time": "2022-10-29T16:42:00.969917",
     "exception": false,
     "start_time": "2022-10-29T16:42:00.813274",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor({\"CPU:0\": [[0.0368092842 1.76192284]\n",
      " [1.22868407 -0.731756687]], \"CPU:1\": [[0.255247623 -0.13820985]\n",
      " [-0.747412503 1.06443202]], \"CPU:2\": [[-0.395325899 -0.836183369]\n",
      " [0.581941128 -0.2587713]], \"CPU:3\": [[0.476060659 0.406645179]\n",
      " [-0.110623844 -1.49052978]], \"CPU:4\": [[0.645035267 1.36384416]\n",
      " [2.18210244 -0.965060234]], \"CPU:5\": [[-1.70534277 1.32558191]\n",
      " [0.972473264 0.972343624]]}, layout=\"sharding_specs:x,y, mesh:|x=3,y=2|0,1,2,3,4,5|0,1,2,3,4,5|/job:localhost/replica:0/task:0/device:CPU:0,/job:localhost/replica:0/task:0/device:CPU:1,/job:localhost/replica:0/task:0/device:CPU:2,/job:localhost/replica:0/task:0/device:CPU:3,/job:localhost/replica:0/task:0/device:CPU:4,/job:localhost/replica:0/task:0/device:CPU:5\", shape=(6, 4), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-29 16:42:00.907815: I tensorflow/dtensor/cc/dtensor_device.cc:1390] DTensor cache key lookup missed for __inference_stateless_random_normal_114. DTensor is (re-)computing its SPMD transformation.\n",
      "2022-10-29 16:42:00.942521: E tensorflow/core/framework/node_def_util.cc:630] NodeDef mentions attribute dtensor.device_seed_for_mesh_dims which is not in the op definition: Op<name=Squeeze; signature=input:T -> output:T; attr=T:type; attr=squeeze_dims:list(int),default=[],min=0> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node tf.StatefulPartitionedCall/tf.Squeeze}}\n"
     ]
    }
   ],
   "source": [
    "ones = dtensor.call_with_layout(\n",
    "    tf.function(tf.random.stateless_normal),\n",
    "    dtensor.Layout(['x', 'y'], mesh),\n",
    "    shape=(6, 4),\n",
    "    seed=(1, 1))\n",
    "print(ones)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf76d78",
   "metadata": {
    "papermill": {
     "duration": 0.068942,
     "end_time": "2022-10-29T16:42:01.108230",
     "exception": false,
     "start_time": "2022-10-29T16:42:01.039288",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Wrapping a Python function that emits a single TensorFlow Op with `tf.function` is allowed. The only caveat is paying the associated cost and complexity of creating a `tf.function` from a Python function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6dd9a01f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-29T16:42:01.249236Z",
     "iopub.status.busy": "2022-10-29T16:42:01.247984Z",
     "iopub.status.idle": "2022-10-29T16:42:01.277654Z",
     "shell.execute_reply": "2022-10-29T16:42:01.276407Z"
    },
    "papermill": {
     "duration": 0.103005,
     "end_time": "2022-10-29T16:42:01.280780",
     "exception": false,
     "start_time": "2022-10-29T16:42:01.177775",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor({\"CPU:0\": [[1 1]\n",
      " [1 1]], \"CPU:1\": [[1 1]\n",
      " [1 1]], \"CPU:2\": [[1 1]\n",
      " [1 1]], \"CPU:3\": [[1 1]\n",
      " [1 1]], \"CPU:4\": [[1 1]\n",
      " [1 1]], \"CPU:5\": [[1 1]\n",
      " [1 1]]}, layout=\"sharding_specs:x,y, mesh:|x=3,y=2|0,1,2,3,4,5|0,1,2,3,4,5|/job:localhost/replica:0/task:0/device:CPU:0,/job:localhost/replica:0/task:0/device:CPU:1,/job:localhost/replica:0/task:0/device:CPU:2,/job:localhost/replica:0/task:0/device:CPU:3,/job:localhost/replica:0/task:0/device:CPU:4,/job:localhost/replica:0/task:0/device:CPU:5\", shape=(6, 4), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-29 16:42:01.257478: I tensorflow/dtensor/cc/dtensor_device.cc:1390] DTensor cache key lookup missed for __inference_ones_120. DTensor is (re-)computing its SPMD transformation.\n"
     ]
    }
   ],
   "source": [
    "ones = dtensor.call_with_layout(\n",
    "    tf.function(tf.ones),\n",
    "    dtensor.Layout(['x', 'y'], mesh),\n",
    "    shape=(6, 4))\n",
    "print(ones)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4882878",
   "metadata": {
    "papermill": {
     "duration": 0.069141,
     "end_time": "2022-10-29T16:42:01.419200",
     "exception": false,
     "start_time": "2022-10-29T16:42:01.350059",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <div style=\"font-family: Trebuchet MS; background-color: #D68910; color: #FFFFFF; padding: 12px; line-height: 1.5;border-radius:15px 60px 15px; text-align:center;border-color: red;border: 5px solid #17b978;overflow:hidden;\">9. | From `tf.Variable` to `dtensor.DVariable`</div>\n",
    "\n",
    "In Tensorflow, `tf.Variable` is the holder for a mutable `Tensor` value.\n",
    "With DTensor, the corresponding variable semantics is provided by `dtensor.DVariable`.\n",
    "\n",
    "The reason a new type `DVariable` was introduced for DTensor variable is because DVariables have an additional requirement that the layout cannot change from its initial value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "707f6c33",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-29T16:42:01.560315Z",
     "iopub.status.busy": "2022-10-29T16:42:01.559684Z",
     "iopub.status.idle": "2022-10-29T16:42:01.650315Z",
     "shell.execute_reply": "2022-10-29T16:42:01.648861Z"
    },
    "papermill": {
     "duration": 0.164219,
     "end_time": "2022-10-29T16:42:01.653050",
     "exception": false,
     "start_time": "2022-10-29T16:42:01.488831",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(<ResourceHandle(name=\"Resource-0-at-0x7f821c00f570\", device=\"/job:localhost/replica:0/task:0/device:CPU:0\", container=\"Anonymous\", type=\"tensorflow::Var\", dtype and shapes : \"[ DType enum: 1, Shape: [64,32] ]\")>, layout=\"sharding_specs:unsharded,unsharded, mesh:|x=6|0,1,2,3,4,5|0,1,2,3,4,5|/job:localhost/replica:0/task:0/device:CPU:0,/job:localhost/replica:0/task:0/device:CPU:1,/job:localhost/replica:0/task:0/device:CPU:2,/job:localhost/replica:0/task:0/device:CPU:3,/job:localhost/replica:0/task:0/device:CPU:4,/job:localhost/replica:0/task:0/device:CPU:5\", shape=(), dtype=resource)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-29 16:42:01.577051: I tensorflow/dtensor/cc/dtensor_device.cc:1390] DTensor cache key lookup missed for __inference_stateless_random_normal_135. DTensor is (re-)computing its SPMD transformation.\n"
     ]
    }
   ],
   "source": [
    "mesh = dtensor.create_mesh([(\"x\", 6)], devices=DEVICES)\n",
    "layout = dtensor.Layout([dtensor.UNSHARDED, dtensor.UNSHARDED], mesh)\n",
    "\n",
    "v = dtensor.DVariable(\n",
    "    initial_value=dtensor.call_with_layout(\n",
    "        tf.function(tf.random.stateless_normal),\n",
    "        layout=layout,\n",
    "        shape=tf.TensorShape([64, 32]),\n",
    "        seed=[1, 1],\n",
    "        dtype=tf.float32))\n",
    "\n",
    "print(v.handle)\n",
    "assert layout == dtensor.fetch_layout(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caab2c53",
   "metadata": {
    "papermill": {
     "duration": 0.069978,
     "end_time": "2022-10-29T16:42:01.799052",
     "exception": false,
     "start_time": "2022-10-29T16:42:01.729074",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Other than the requirement on matching the `layout`, a `DVariable` behaves the same as a `tf.Variable`. For example, you can add a DVariable to a DTensor,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d5acd344",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-29T16:42:01.941320Z",
     "iopub.status.busy": "2022-10-29T16:42:01.940863Z",
     "iopub.status.idle": "2022-10-29T16:42:01.971700Z",
     "shell.execute_reply": "2022-10-29T16:42:01.970308Z"
    },
    "papermill": {
     "duration": 0.104112,
     "end_time": "2022-10-29T16:42:01.974272",
     "exception": false,
     "start_time": "2022-10-29T16:42:01.870160",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[2.66521645 2.36637592 1.77863169 ... -1.18624139 2.26035929 0.664066315]\n",
      " [0.511952519 0.655031443 0.122243524 ... 0.0424078107 1.67057109 0.912334144]\n",
      " [0.769825 1.42743981 3.13473773 ... 1.16159868 0.628931046 0.733521938]\n",
      " ...\n",
      " [0.388001859 2.72882509 2.92771554 ... 1.17472672 1.72462416 1.5047121]\n",
      " [-0.252545118 0.761886716 1.72119033 ... 0.775034547 2.8065362 1.00457215]\n",
      " [1.23498726 0.584536672 1.15659761 ... 0.955793858 1.11440909 0.18848455]], layout=\"sharding_specs:unsharded,unsharded, mesh:|x=6|0,1,2,3,4,5|0,1,2,3,4,5|/job:localhost/replica:0/task:0/device:CPU:0,/job:localhost/replica:0/task:0/device:CPU:1,/job:localhost/replica:0/task:0/device:CPU:2,/job:localhost/replica:0/task:0/device:CPU:3,/job:localhost/replica:0/task:0/device:CPU:4,/job:localhost/replica:0/task:0/device:CPU:5\", shape=(64, 32), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "a = dtensor.call_with_layout(tf.ones, layout=layout, shape=(64, 32))\n",
    "b = v + a # add DVariable and DTensor\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fae7d03",
   "metadata": {
    "papermill": {
     "duration": 0.069162,
     "end_time": "2022-10-29T16:42:02.114226",
     "exception": false,
     "start_time": "2022-10-29T16:42:02.045064",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "You can also assign a DTensor to a DVariable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a320660a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-29T16:42:02.255268Z",
     "iopub.status.busy": "2022-10-29T16:42:02.254788Z",
     "iopub.status.idle": "2022-10-29T16:42:02.270522Z",
     "shell.execute_reply": "2022-10-29T16:42:02.269450Z"
    },
    "papermill": {
     "duration": 0.089122,
     "end_time": "2022-10-29T16:42:02.272975",
     "exception": false,
     "start_time": "2022-10-29T16:42:02.183853",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[1 1 1 ... 1 1 1]\n",
      " [1 1 1 ... 1 1 1]\n",
      " [1 1 1 ... 1 1 1]\n",
      " ...\n",
      " [1 1 1 ... 1 1 1]\n",
      " [1 1 1 ... 1 1 1]\n",
      " [1 1 1 ... 1 1 1]], layout=\"sharding_specs:unsharded,unsharded, mesh:|x=6|0,1,2,3,4,5|0,1,2,3,4,5|/job:localhost/replica:0/task:0/device:CPU:0,/job:localhost/replica:0/task:0/device:CPU:1,/job:localhost/replica:0/task:0/device:CPU:2,/job:localhost/replica:0/task:0/device:CPU:3,/job:localhost/replica:0/task:0/device:CPU:4,/job:localhost/replica:0/task:0/device:CPU:5\", shape=(64, 32), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "v.assign(a) # assign a DTensor to a DVariable\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c71d05",
   "metadata": {
    "papermill": {
     "duration": 0.068814,
     "end_time": "2022-10-29T16:42:02.411543",
     "exception": false,
     "start_time": "2022-10-29T16:42:02.342729",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Attempting to mutate the layout of a `DVariable`, \n",
    "\n",
    "by assigning a DTensor with an incompatible layout produces an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "50d932ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-29T16:42:02.552695Z",
     "iopub.status.busy": "2022-10-29T16:42:02.551891Z",
     "iopub.status.idle": "2022-10-29T16:42:02.575820Z",
     "shell.execute_reply": "2022-10-29T16:42:02.574803Z"
    },
    "papermill": {
     "duration": 0.096989,
     "end_time": "2022-10-29T16:42:02.577981",
     "exception": false,
     "start_time": "2022-10-29T16:42:02.480992",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exception raised\n"
     ]
    }
   ],
   "source": [
    "# variable's layout is immutable.\n",
    "another_mesh = dtensor.create_mesh([(\"x\", 3), (\"y\", 2)], devices=DEVICES)\n",
    "b = dtensor.call_with_layout(tf.ones,\n",
    "                     layout=dtensor.Layout([dtensor.UNSHARDED, dtensor.UNSHARDED], another_mesh),\n",
    "                     shape=(64, 32))\n",
    "try:\n",
    "  v.assign(b)\n",
    "except:\n",
    "  print(\"exception raised\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b17e97",
   "metadata": {
    "papermill": {
     "duration": 0.070978,
     "end_time": "2022-10-29T16:42:02.719225",
     "exception": false,
     "start_time": "2022-10-29T16:42:02.648247",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"font-weight:bold;font-size:20px;line-height:1.5;padding:12px;background-color: #f9ff21;font-family: Verdana, Cursive; color: #ff1f5a;border: 5px solid #17b978;\">Conclusion: <li style=\"font-size:15px;font-weight:normal;margin-bottom:5px\" >\n",
    "    For using <code>DTensor</code> we need Tensorflow version 2.9.0 and above.\n",
    "</li>\n",
    "<li style=\"font-size:15px;font-weight:normal;margin-bottom:5px\">\n",
    "DTensor enables running the same application on a single device, multiple devices, or even multiple clients, while preserving its global semantics.\n",
    "</li>\n",
    "<li style=\"font-size:15px;font-weight:normal;margin-bottom:5px\">\n",
    "Using DTensor we can train our ML Model on distributed system which can be very efficient use of computation.\n",
    "</li>\n",
    "<li style=\"font-size:15px;font-weight:normal;margin-bottom:5px\">\n",
    "DTensor uses Mesh & Layout where Mesh is a logical topology and Layout is how Tensor is distributed on mesh.\n",
    "</li>\n",
    "    <li style=\"font-size:15px;font-weight:normal;margin-bottom:5px\">\n",
    "        We can convert an array to a DTensor using <code>dtensor.copy_to_mesh()</code>\n",
    "</li>\n",
    "      <li style=\"font-size:15px;font-weight:normal;margin-bottom:5px\">\n",
    "We can create a fully sharded, fully replicated or hybrid type of dtensor. \n",
    "</li>\n",
    "     <li style=\"font-size:15px;font-weight:normal;margin-bottom:5px\">\n",
    "         We can use operations like <code>tf.matmul()</code> by passing dtensor operands.\n",
    "</li>\n",
    "    <li style=\"font-size:15px;font-weight:normal;margin-bottom:5px\">\n",
    "We can use get output in form of dtensor from tensorflow functions using <code>dtensor.call_with_layout</code>\n",
    "</li>\n",
    "      <li style=\"font-size:15px;font-weight:normal;margin-bottom:5px\">\n",
    "          Similar to <code>tf.Variable()</code> we can use <code>tf.DVariable()</code> for dtensor.\n",
    "</li>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f00c3d4",
   "metadata": {
    "papermill": {
     "duration": 0.071335,
     "end_time": "2022-10-29T16:42:02.861328",
     "exception": false,
     "start_time": "2022-10-29T16:42:02.789993",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"font-weight:bold;font-size:20px;line-height:1.5;padding:12px;background-color: #f9ff21;font-family: Verdana, Cursive; color: #ff1f5a;border: 5px solid #17b978;\">References: <li style=\"font-size:15px;font-weight:normal;margin-bottom:5px\" >\n",
    "    <a href=\"https://www.tensorflow.org/guide/dtensor_overview\">https://www.tensorflow.org/guide/dtensor_overview</a>\n",
    "</li>\n",
    "    <li style=\"font-size:15px;font-weight:normal;margin-bottom:5px\">\n",
    "    <a href=\"https://github.com/tensorflow/docs/blob/master/site/en/guide/dtensor_overview.ipynb\">https://github.com/tensorflow/docs/blob/master/site/en/guide/dtensor_overview.ipynb</a>\n",
    "</li>\n",
    "     <li style=\"font-size:15px;font-weight:normal;margin-bottom:5px\" >\n",
    "    <a href=\"https://www.infoq.com/news/2022/05/tensorflow-dtensor/\">  https://www.infoq.com/news/2022/05/tensorflow-dtensor</a>\n",
    "</li>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a135de3",
   "metadata": {
    "papermill": {
     "duration": 0.068939,
     "end_time": "2022-10-29T16:42:03.000001",
     "exception": false,
     "start_time": "2022-10-29T16:42:02.931062",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"font-weight:bold;font-size:20px;line-height:1.5;padding:12px;background-color: #f9ff21;font-family: Verdana, Cursive; color: #ff1f5a;border: 5px solid #17b978;\">What Next? \n",
    "    <br>\n",
    "    <div style=\"font-size:15px\">\n",
    "In the next Update of this notebook, I will share how you can Train a Tensorflow model using DTensor. Wait for the Update. 🤞🤞\n",
    "    </div>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c9027b",
   "metadata": {
    "papermill": {
     "duration": 0.069033,
     "end_time": "2022-10-29T16:42:03.138498",
     "exception": false,
     "start_time": "2022-10-29T16:42:03.069465",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"font-weight:bold;font-size:20px;line-height:1.5;padding:12px;background-color: #f9ff21;font-family: Verdana, Cursive; color: #ff1f5a;border: 5px solid #17b978;text-align:center\">\n",
    "   🔥👍 If you like my work don't forget to appreciate. 👍🔥\n",
    "    <br>\n",
    "    📝 Give you valuable comments 📝\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 135.347265,
   "end_time": "2022-10-29T16:42:06.721073",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-10-29T16:39:51.373808",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
